#!/usr/bin/env python3
# vim: set ft=python sts=4 ts=4 sw=4 et:
# BSD 2-clause license. Author: Adam Wojciech Koszek <adam@koszek.com>

# @todo: we should unify the interface for all the models so
# that we use the same OpenAI interface for all the models.

import sys
import time
import datetime
import argparse
import re
import os
from openai import OpenAI
from anthropic import Anthropic
from google import generativeai 

# Initialize API clients
openai_client = OpenAI()
anthropic_client = Anthropic()
generativeai.configure()

def format_yaml_output(prompt_unindented, output_unindented, provider, model, time_sent_at, time_recv_at):
    """Format the output as a YAML string with literal style for text."""    
    prompt = "\n".join([f"    {line.strip()}" for line in prompt_unindented.split("\n")])
    output = "\n".join([f"    {line.strip()}" for line in output_unindented.split("\n")])
    yaml_str = f"""sent:
  time_sent_at: {time_sent_at}
  prompt: |
{prompt}
  provider: {provider}
  model: {model}
received:
  time_recv_at: {time_recv_at}
  output: |
{output}
  provider: {provider}
  model: {model}
"""
    return yaml_str

def call_openai_model(prompt, model):
    """Calls OpenAI API with the given prompt and model."""
    start_time = time.time()
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    output_text = response.choices[0].message.content
    end_time = time.time()
    return output_text, start_time, end_time

def call_anthropic_model(prompt, model):
    """Calls Anthropic API with the given prompt and model."""
    start_time = time.time()
    response = anthropic_client.messages.create(
        model=model,
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )
    output_text = response.content[0].text
    end_time = time.time()
    return output_text, start_time, end_time

def call_gemini_model(prompt, model):
    """Calls Google's Gemini API with the given prompt and model."""
    start_time = time.time()
    model = generativeai.GenerativeModel(model)
    response = model.generate_content(prompt)
    output_text = response.text
    end_time = time.time()
    return output_text, start_time, end_time

def call_grok_model(prompt, model):
    """Calls Grok API using OpenAI SDK with x.ai endpoint."""
    start_time = time.time()
    
    # Get API key from environment
    api_key = os.getenv('GROK_API_KEY')
    if not api_key:
        raise ValueError("GROK_API_KEY environment variable is not set")
    
    # Create a new OpenAI client with x.ai endpoint
    grok_client = OpenAI(
        api_key=api_key,
        base_url="https://api.x.ai/v1"
    )
    
    # Call the API using OpenAI SDK
    response = grok_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    
    output_text = response.choices[0].message.content
    end_time = time.time()
    return output_text, start_time, end_time

def main():
    # Get the program's basename
    program_name = os.path.basename(sys.argv[0])
    
    # Map provider to backend function
    provider_map = {
        'openai': call_openai_model,
        'anthropic': call_anthropic_model,
        'google': call_gemini_model,
        'xai': call_grok_model
    }

    # Check if program name follows lmt-PROVIDER-MODEL pattern
    if program_name.startswith("lmt-"):
        match = re.match(r'lmt-([^-]+)-(.*)', program_name)
        if match:
            provider = match.group(1)
            model = match.group(2)
            # Validate provider
            if provider not in provider_map.keys():
                print(f"Error: Invalid provider '{provider}' in symlink name. Must be one of: openai, anthropic, google, xai", file=sys.stderr)
                sys.exit(1)
            # Create a custom argument list for argparse
            sys.argv = [sys.argv[0], provider, model]
    
    parser = argparse.ArgumentParser(description="Unified Language Model CLI")
    parser.add_argument("provider", type=str, choices=['openai', 'anthropic', 'google', 'xai'],
                      help="LLM provider to use (openai, anthropic, google, xai)")
    parser.add_argument("model", type=str, help="Model name to use (e.g., gpt-4, claude-3-opus-20240229, gemini-pro, grok-1)")
    args = parser.parse_args()

    # Get the backend function
    backend_func = provider_map[args.provider]

    # Read input from stdin
    prompt = sys.stdin.read()
    if not prompt:
        print("Error: No input provided on STDIN.", file=sys.stderr)
        sys.exit(1)

    try:
        # Call the appropriate model
        output, time_sent_at, time_recv_at = backend_func(prompt, args.model)

        # Print output to STDOUT
        print(output)

        # Generate log filename with seconds
        timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S-%f")
        log_filename = f"{timestamp}-{args.provider}-{args.model}.yaml"

        # Format and write YAML output
        yaml_output = format_yaml_output(
            prompt, output, args.provider, args.model,
            time_sent_at, time_recv_at
        )

        with open(log_filename, "w") as log_file:
            log_file.write(yaml_output)

    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main() 
